{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import json\n",
    "OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')\n",
    "import langchain\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "embedded_res = embeddings_model.embed_query(\"What was the name mentioned in the conversation?\")\n",
    "embedded_res[:5]\n",
    "\n",
    "len(embedded_res)\n",
    "# print (embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import pickle\n",
    "import hashlib\n",
    "hashlib.sha256(b\"test text\").hexdigest()\n",
    "EMBEDDING_LEN = 1536 # OPENAI EMBEDDING LEN\n",
    "kv_cache = pickle.load(open('kv_cache_openai_embedding.pickle', 'rb'))\n",
    "def insert_embedding(data_file):\n",
    "    # data_file = 'fp_dict_with_code_with_ir.json'\n",
    "    data_file = \"../smartagent-dataset/Scripts/gpt3.5_fp_with_code.json\"\n",
    "    fp_data = json.load(open(data_file))\n",
    "    count = 0\n",
    "    for key_,val_ in fp_data.items():\n",
    "        for func_name, func_info in val_.items():\n",
    "            print (func_name)\n",
    "            if \"code_embedding\" in func_info:\n",
    "                print (\"found embedding \", len(func_info['code_embedding']))\n",
    "                continue\n",
    "            if 'code' in func_info:\n",
    "                solidity_code = func_info['code'].get(\"raw\")\n",
    "                if (solidity_code is None) or (len(solidity_code) == 0):\n",
    "                    continue\n",
    "                # print (code)\n",
    "                code_embedding = embeddings_model.embed_query(solidity_code)\n",
    "                # print (hashlib.sha256(solidity_code.encode()).hexdigest())\n",
    "                kv_cache[hashlib.sha256(solidity_code.encode()).hexdigest()] = code_embedding\n",
    "                if code_embedding and len(code_embedding) != 0:\n",
    "                    func_info['code_embedding'] = code_embedding\n",
    "                    count += 1\n",
    "        # if count > 5:\n",
    "        #     break\n",
    "        write_file = data_file.split('.')[0] + '_with_embedding_code.json'\n",
    "        json.dump(fp_data, open(write_file, 'w'), indent=2)\n",
    "\n",
    "insert_embedding('fp_dict_with_code_with_ir.json')\n",
    "\n",
    "pickle.dump(kv_cache, open('kv_cache_openai_embedding.pickle', 'wb'))\n",
    "\n",
    "# print (list(fp_data.values())[:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge json files and remove ir\n",
    "import json\n",
    "file_1 = \"gpt_fp_with_embedding_code.json\"\n",
    "file_2 = \"fp_dict_with_code_ir_embedding_filtered.json\"\n",
    "final_file = \"all_fp_data_with_embedding_merge.json\"\n",
    "with open(file_1) as f:\n",
    "    data_1 = json.load(f)\n",
    "with open(file_2) as f:\n",
    "    data_2 = json.load(f)\n",
    "\n",
    "all_fp_data = {}\n",
    "\n",
    "for key_,val_ in file_1.items():\n",
    "    new_val = {}\n",
    "    for func_name, func_info in val_.items():\n",
    "        if \"code_embedding\" not in func_info:\n",
    "            print (\"not found embedding \", func_name)\n",
    "            continue\n",
    "        else:\n",
    "            new_val[func_name] = func_info\n",
    "    if len(new_val) != 0:\n",
    "        all_fp_data[key_] = new_val\n",
    "for key_,val_ in file_2.items():\n",
    "    if key_ in all_fp_data:\n",
    "        new_val = all_fp_data[key_]\n",
    "    else:\n",
    "        new_val = {}\n",
    "    for func_name, func_info in val_.items():\n",
    "        if \"code_embedding\" not in func_info:\n",
    "            print (\"not found embedding \", func_name)\n",
    "            continue\n",
    "        else:\n",
    "            new_val[func_name] = func_info\n",
    "    if len(new_val) != 0:\n",
    "        all_fp_data[key_] = new_val\n",
    "\n",
    "\n",
    "json.dump(all_fp_data, open(final_file, 'w'), indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test_data from pickle\n",
    "import pickle\n",
    "import json\n",
    "# test_data = pickle.load(open('kv_cache_openai_embedding.pickle', 'rb'))\n",
    "# print (list(test_data.values())[0])\n",
    "# print (list(test_data.values())[1])\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn import preprocessing\n",
    "from scipy import stats\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "tools = ['mythril', 'slither']\n",
    "from sklearn.cluster import KMeans\n",
    "# print (test_data)\n",
    "from collections import Counter\n",
    "\n",
    "def extract_tool_embedding(data_file, tool, embedding_type = 'code_embedding'):\n",
    "    #data_file = 'fp_dict_with_code_with_ir_with_embedding_code_ir.json'\n",
    "    fp_data = json.load(open(data_file))\n",
    "    count = 0\n",
    "    return_data = []\n",
    "    for key_,val_ in fp_data.items():\n",
    "        for func_name, func_info in val_.items():\n",
    "            # print (func_name)\n",
    "            if embedding_type in func_info and tool in func_info.get('tools', []):\n",
    "                count = count + 1\n",
    "                return_data.append(func_info.get(embedding_type))\n",
    "    print (\"found \", count, \" functions with \", tool, embedding_type)\n",
    "    return return_data\n",
    "# res = extract_tool_embedding('fp_dict_with_code_with_ir_with_embedding_code_ir.json', 'slither', 'ir_embedding')\n",
    "\n",
    "def plot_tool(tool_name, embedding_type, data_file ):\n",
    "    data = extract_tool_embedding(data_file, tool_name, embedding_type)\n",
    "\n",
    "    matrix = np.array(data)\n",
    "    normalized_matrix =  preprocessing.normalize(matrix)\n",
    "\n",
    "    if tool_name == 'mythril':\n",
    "        n_clusters = 5\n",
    "    elif tool_name == 'gpt_3.5':\n",
    "        n_clusters = 5\n",
    "    else:\n",
    "        n_clusters = 5\n",
    "    kmeans = KMeans(n_clusters=n_clusters, init=\"k-means++\", random_state=42)\n",
    "    kmeans.fit(normalized_matrix)\n",
    "    labels = kmeans.labels_\n",
    "    cluster_counts = Counter(labels)\n",
    "    # print (\"label data\")\n",
    "    for idx, item in enumerate(labels):\n",
    "        # print (item)\n",
    "        if item == 3:\n",
    "            print (idx)\n",
    "    # Print out the count of each cluster\n",
    "    for cluster_id, count in cluster_counts.items():\n",
    "        print(f\"Cluster {cluster_id}: {count} items\")\n",
    "\n",
    "    # Step 1: Compute distances of each point to the centroids\n",
    "    distances = kmeans.transform(normalized_matrix)\n",
    "\n",
    "    # Step 2: Find the index of the closest point to each centroid\n",
    "    closest_points_indices = np.argmin(distances, axis=0)\n",
    "\n",
    "    # Extract the closest points\n",
    "    # closest_points = normalized_matrix[closest_points_indices]\n",
    "    print (\"closest points \")\n",
    "    print (closest_points_indices)\n",
    "    print(\"data shape\", matrix.shape)\n",
    "    # print(\"data sample\", matrix[:2][:5])\n",
    "    # tsne = TSNE(n_components=2,metric=\"cosine\", random_state=42, init=\"random\")\n",
    "    tsne = TSNE(n_components=2, perplexity=10,metric=\"cosine\", random_state=42, init=\"random\", learning_rate=200)\n",
    "    vis_dims2 = tsne.fit_transform(matrix)\n",
    "    x = [x for x, y in vis_dims2]\n",
    "    y = [y for x, y in vis_dims2]\n",
    "    print (\"max min x idx \",np.argmax(x), np.argmin(x))\n",
    "    print (\"max min x val \",np.max(x), np.min(x))\n",
    "    print (\"max min y idx \",np.argmax(y), np.argmin(y))\n",
    "    print (\"max min y val \",np.max(y), np.min(y))\n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(4*1.618,4))\n",
    "    colormap = plt.cm.get_cmap('viridis', n_clusters)  # Replace 'viridis' with any other colormap if needed\n",
    "    for category in range(n_clusters):\n",
    "        color = colormap(category)\n",
    "        # Correct the filtering here\n",
    "        xs = np.array(x)[labels == category]\n",
    "        ys = np.array(y)[labels == category]\n",
    "        plt.scatter(xs, ys, color=color, alpha=0.3)\n",
    "\n",
    "        if len(xs) > 0:\n",
    "            avg_x = xs.mean()\n",
    "            avg_y = ys.mean()\n",
    "            plt.scatter(avg_x, avg_y, marker=\"x\", color=color, s=100)\n",
    "\n",
    "    # plt.title(\"Clusters visualization with t-SNE\")\n",
    "    plt.savefig(f\"{tool_name}_{embedding_type}_cluster.pdf\")\n",
    "    return closest_points_indices\n",
    "\n",
    "plot_tool('gpt_3.5', 'code_embedding', \"merged_fp_all_tools.json\")\n",
    "plot_tool('slither', 'code_embedding', \"merged_fp_all_tools.json\")\n",
    "plot_tool('mythril', 'code_embedding', \"merged_fp_all_tools.json\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# extract centroid representative code\n",
    "def extract_centroid_code(data_file, tool, centroid_indices, embedding_type = 'code_embedding'):\n",
    "    fp_data = json.load(open(data_file))\n",
    "    count = 0\n",
    "    return_data = []\n",
    "    for key_,val_ in fp_data.items():\n",
    "        for func_name, func_info in val_.items():\n",
    "            # print (func_name)\n",
    "            if embedding_type in func_info and tool in func_info.get('tools', []):\n",
    "                if count in centroid_indices:\n",
    "                    print (\"found \", count, \" functions with \", tool, embedding_type)\n",
    "                    return_data.append({\n",
    "                        \"key\" : key_,\n",
    "                        \"func_name\" : func_name,\n",
    "                        \"func_info\" : func_info,\n",
    "                    })\n",
    "                # return_data.append(func_info.get(embedding_type))\n",
    "                count = count + 1\n",
    "    print (\"found centroids \", len(return_data), \" functions with \", tool, embedding_type)\n",
    "    json.dump(return_data, open(f\"{tool}_fp_centroids.json\", 'w'), indent=2)\n",
    "    return return_data\n",
    "extract_centroid_code('merged_fp_all_tools.json', 'slither', slither_centroid_indices, 'code_embedding')\n",
    "extract_centroid_code('merged_fp_all_tools.json', 'mythril', mythril_centroid_indices, 'code_embedding')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
